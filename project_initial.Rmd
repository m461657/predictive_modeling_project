---
title: "project_initial"
output: html_document
---

What is a linear predictive model?
Non-linear predictive model?

install packages
```{r}
install.packages("tidytuesdayR")

install.packages("ggpmisc")
```
Load packages
```{r}
library(tidyverse)
library(readr)
library(ggplot2)
library(ggthemes)
```

Import data
```{r}
friends <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv')
friends_emotions <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_emotions.csv')
friends_info <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_info.csv')
food_consumption <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv')

```

```{r}
#Simple LR model
ggplot(data = Fish, aes(x = Length3, y = Weight))+
  geom_point()+
  geom_abline(aes(x = Length3, y = Weight), data = Fish, slope = 28.46, intercept = -490.40)

lm1 = lm(Weight~Length3, data = Fish)

#weight in g, length in cm
```

```{r}
#MR model
##for activity, have them compare all three models (one with l*w*h)

lm2 = lm(Weight~Length3 + Height, data = Fish)

lm3 = lm(Weight~Length3 + Height + Width, data = Fish)
#coefficients of determination
summary(lm1)$r.sq
summary(lm2)$r.sq
summary(lm3)$r.sq

Fish_new = Fish %>% 
  select(Weight, Length3, Height, Weight)

cor(Fish_new)
```


```{r}
#View(friends)
View(friends_info)
```
Linear relationship between US views and airdate? 

```{r}
ggplot(data = friends_info, aes(x = air_date, y = us_views_millions))+
  geom_point()+
  geom_smooth(se = FALSE)
```

#What is predictive modeling?
**Predictive modeling** is the process of using known data and results, to create and analyze a model to predict future outcomes.

The goal is to answer the question "What could possibly happen in the future?"

# Linear predictive models

There are a few different types of linear predictive models, but today I will talk about Simple Linear Regression and Multiple Regression.

## Simple Linear Regression
*ŷ = b0 + b1x*

*ŷ* --> the predicted dependent variable
*b0* --> the intercept of the line
*x* --> the independent variable (the variable we are using to predict ŷ)
*b1* --> the slope of the line

This equation is the same as y=mx+b, just written using statistical terms instead
The goal of linear regression, a linear predictive model, is to find the "line of best fit" by minimizing the squared error **more on that later**

**have example of a linear model, graphed and with equation output**
## Multiple Regression
Can be both linear and non-linear
It is very rare a dependent variable is only explained by *one* independent variable. So, we can use multiple regression instead!

*ŷ = b0 + b1x1 + b2x2*

*ŷ* = the predicted dependent variable
*b0* = the intercept of the line
*x1* = an independent variable
*b1* = regression coefficient
*x2* = another independent variable
*b2* = another regression coefficient

**have an example, graphed with equation output**

# Measuring the Accuracy of a Linear Predictive Model

There are multiple ways to measure the accuracy of a predictive model, based on the set of data you are given. The more accurate the model, the better it will do at predicting future events.
**Correlation Coefficient**
One very basic measurement of "fitness" for a model is the correlation coefficient. The correlation coefficient (r) is a measure of the strength of the linear relationship between two variables. It also gives the direction of the relationship (positive or negative).
**Coefficient of Determination**
Denoted as r^2, the coefficient of determination is interpreted as the proportion of the variance in the dependent variable(ŷ) that is predictable from the independent variable(s). That is, how much of ŷ is explained by the x('s). It is the square of the correlation coefficient.
**Residuals**
The residuals are the actual value, minus the predicted value, for each observation. The smaller the |residual| is, the closer that observation falls to the predictive model.

*res = (Y-Yest)*
*Y* = observed value
*Yest* = predicted value

**Have activity where they calculate the residuals**
Activity 1
```{r, eval = FALSE}
data_ex = data.frame("x" = c(1, 2, 4, 6, 7), "y" = c(1, 2.5, 6, 5, 4.5))

ggplot(data = data_ex, aes(x = x, y = y))+
  geom_point(aes(color = "hotpink"), size = 3.5)+
  geom_abline(mapping = NULL, data = NULL, slope = 1, intercept = 0, aes(color = "darkgray"), size = 1.5)+
  scale_x_continuous(name = "", limits = c(0,8), breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8))+
  scale_y_continuous(name = "", limits = c(0,8), breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8))+
  theme_economist_white()+
  theme(legend.position ="none")+
  geom_text(label = "y = x", x = 4, y = 3.25, size = 7)+
  geom_text(label = "(4,6)", x = 4, y = 6.5)+
  geom_text(label = "(2,2.5)", x = 2, y = 3)+
  geom_text(label = "(1,1)", x = 1, y = 1.5)+
  geom_text(label = "(6,5)", x = 6, y = 4.5)+
  geom_text(label = "(7,5)", x = 7, y = 5)
```
**Residual Standard Error**
The residual standard error (or residual standard deviation) is the measure of the standard deviation of the residuals. The equation is:

*Sres = sqrt[sum(Y-Yest)^2/n-2]*

*Sres* --> residual standard deviation
*Y* --> observed value
*Yest* --> predicted value
*n* --> number of observations

All of these different methods can be used to compare models, and determine which model is the *best* predictive model! There are many other methods used to compare models, so these are just a few.
  
  **correlation matrix**
  
**go into group activity** 

# Non-Linear Predictive Models




