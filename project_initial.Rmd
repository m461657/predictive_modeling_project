---
title: "project_initial"
output: html_document
---

What is a linear predictive model?
Non-linear predictive model?

**TESTING AND TRAINING SETS!!**

Load packages
```{r}
library(tidyverse)
library(readr)
library(ggplot2)
library(ggthemes)
```

Import data
```{r, eval = FALSE}
friends <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv')
friends_emotions <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_emotions.csv')
friends_info <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_info.csv')
food_consumption <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv')
```

```{r}
#View(friends)
View(friends_info)
```
Linear relationship between US views and airdate? 

```{r}
ggplot(data = friends_info, aes(x = air_date, y = us_views_millions))+
  geom_point()+
  geom_smooth(se = FALSE)
```

#What is predictive modeling?
**Predictive modeling** is the process of using known data and results, to create and analyze a model to predict future outcomes.

The goal is to answer the question "What could possibly happen in the future?"

# Linear predictive models

There are a few different types of linear predictive models, but today I will talk about Simple Linear Regression and Multiple Regression.

## Simple Linear Regression
*ŷ = b0 + b1x*

*ŷ* --> the predicted dependent variable
*b0* --> the intercept of the line
*x* --> the independent variable (the variable we are using to predict ŷ)
*b1* --> the slope of the line

This equation is the same as y=mx+b, just written using statistical terms instead
The goal of linear regression, a linear predictive model, is to find the "line of best fit" by minimizing the squared error 

### Example

```{r}
lm1 = lm(Weight~Length3, data = Fish)
lm1
```
Our simple linear regression equation is:

*Weight = -490.4 + 28.46(Length3)*

So, our intercept is **-490.4** and our slope is **28.46**

```{r}
#Simple LR model
ggplot(data = Fish, aes(x = Length3, y = Weight))+
  geom_point(color = "mediumpurple4")+
  geom_abline(aes(x = Length3, y = Weight), data = Fish, slope = 28.46, intercept = -490.40)+
  labs(x = "Length (in cm)", y = "Weight (in g)", title = "Weight vs. Length of Fish")
```


## Multiple Regression
Can be both linear and non-linear
It is very rare a dependent variable is only explained by *one* independent variable. So, we can use multiple regression instead!

*ŷ = b0 + b1x1 + b2x2*

*ŷ* = the predicted dependent variable
*b0* = the intercept of the line
*x1* = an independent variable
*b1* = regression coefficient
*x2* = another independent variable
*b2* = another regression coefficient

### Example
```{r}
lm2 = lm(Weight~Length3 + Height, data = Fish)
lm2
```
Our multiple regression equation is:

*Weight = -501.1 + 25.24(Length3) + 12.14(Height)*


# Measuring the Accuracy/"Fitness" of a Linear Predictive Model

There are multiple ways to measure the accuracy of a predictive model, based on the set of data you are given. The more accurate the model, the better it will do at predicting future events.
**Correlation Coefficient**
One very basic measurement of "fitness" for a model is the correlation coefficient. The correlation coefficient (r) is a measure of the strength of the linear relationship between two variables. It also gives the direction of the relationship (positive or negative).
**Coefficient of Determination**
Denoted as r^2, the coefficient of determination is interpreted as the proportion of the variance in the dependent variable(ŷ) that is predictable from the independent variable(s). That is, how much of ŷ is explained by the x('s). It is the square of the correlation coefficient.
**Residuals**
Residuals are the measure of how far away each point in a dataset is from the predicted model. The smaller the |residual| is, the closer that observation falls to the predictive model.

*res = (Y-Yest)*
*Y* = observed value
*Yest* = predicted value

```{r, eval = FALSE}
# Activity 1
data_ex = data.frame("x" = c(1, 2, 4, 6, 7), "y" = c(1, 2.5, 6, 5, 4.5))

ggplot(data = data_ex, aes(x = x, y = y))+
  geom_point(aes(color = "hotpink"), size = 3.5)+
  geom_abline(mapping = NULL, data = NULL, slope = 1, intercept = 0, aes(color = "darkgray"), size = 1.5)+
  scale_x_continuous(name = "", limits = c(0,8), breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8))+
  scale_y_continuous(name = "", limits = c(0,8), breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8))+
  theme_economist_white()+
  theme(legend.position ="none")+
  geom_text(label = "y = x", x = 4, y = 3.25, size = 7)+
  geom_text(label = "(4,6)", x = 4, y = 6.5)+
  geom_text(label = "(2,2.5)", x = 2, y = 3)+
  geom_text(label = "(1,1)", x = 1, y = 1.5)+
  geom_text(label = "(6,5)", x = 6, y = 4.5)+
  geom_text(label = "(7,5)", x = 7, y = 5)
```
**Residual Standard Error**
The residual standard error (or residual standard deviation) is the measure of the standard deviation of the residuals. The equation is:

*Sres = sqrt[sum(Y-Yest)^2/n-2]*

*Sres* --> residual standard deviation
*Y* --> observed value
*Yest* --> predicted value
*n* --> number of observations

All of these different methods can be used to compare models, and determine which model is the *best* predictive model! There are many other methods used to compare models, so these are just a few.

```{r}
# Simple linear regression (lm1)
lm1 = lm(Weight~Length3, data = Fish)
lm1
summary(lm1)$r.sq
```

```{r}
# Multiple regression (lm2)
lm2 = lm(Weight~Length3 + Height, data = Fish)
lm2
summary(lm2)$r.sq
```

```{r}
# Multiple regression (lm3)
lm3 = lm(Weight~Length3 + Height + Width, data = Fish)
lm3
summary(lm3)$r.sq
```

```{r, eval = FALSE}
Fish_new = Fish %>% 
  select(Weight, Length3, Height, Weight)
```

```{r}
cor(Fish_new)
```


# Non-Linear Predictive Models

There are many different kinds of non-linear predictive models. From logistic models, to log-least squares regression, and decision trees and random forests just to name a few. 

## Decision Trees
Decision trees are used for categorical variables. 

# Euclidean Norm

The equation for euclidean norm is:

sqrt(∑(Ŷ-Y)^2)

This equation is part of the mean squared error equation, which is another way to measure the accuracy and fitness of a predictive model. MSE tells you how close the regression line is to a set of data points. You take the distance the points are from the line, square them to remove any negatives, and then take the square root. You then take the sum of that value for all the points, and divide by the number of "Y's" you have.  

The equation for mean square error is:

(1/m)∑(Ŷ-Y)^2

Ultimately, the MSE is the normalized euclidean distance. 



