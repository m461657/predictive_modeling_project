---
title: "Linear and Non-Linear Predictive Models"
author: "Megan Willis, MATH 390"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(ggthemes)
Fish <- read.csv("~/Desktop/MATH 390/Project/Fish.csv")
```

## What is predictive modeling?

**Predictive modeling** is the process of using known data and results, to create and analyze a model to predict future outcomes.

The goal is to answer the question "What could possibly happen in the future?"

## What makes a model predictive?

Any type of model can be used as a predictive model. To use a model as a predictive model, you must train and then test the model on a dataset, and then use it to predict future data.

### Data Splitting

Data splitting is when you split data into a training and testing set. The training set is used to build the model, and testing set is used to test the model.


***** 


## Linear predictive models

There are a few different types of linear predictive models, but today I will talk about Simple Linear Regression and Multiple Regression.

### Simple Linear Regression

*ŷ = b0 + b1x*

*ŷ* --> the predicted dependent variable

*b0* --> the intercept of the line

*x* --> the independent variable (the variable we are using to predict ŷ)

*b1* --> the slope of the line

This equation is the same as y=mx+b, just written using statistical terms instead.
The goal of linear regression, a linear predictive model, is to find the "line of best fit" by minimizing the squared error 

**Example**

```{r}
lm1 = lm(Weight~Length3, data = Fish)
lm1
```
Our simple linear regression equation is:

*Weight = -490.4 + 28.46(Length3)*

So, our intercept is **-490.4** and our slope is **28.46**

```{r, warning = FALSE}
#Simple LR model
ggplot(data = Fish, aes(x = Length3, y = Weight))+
  geom_point(color = "mediumpurple4")+
  geom_abline(aes(x = Length3, y = Weight), data = Fish, slope = 28.46, intercept = -490.40)+
  labs(x = "Length (in cm)", y = "Weight (in g)", title = "Weight vs. Length of Fish")
```

### Multiple Regression

Can be both linear and non-linear.It is very rare that a dependent variable is only explained by *one* independent variable. So, we can use multiple regression instead!

*ŷ = b0 + b1x1 + b2x2*

*ŷ* = the predicted dependent variable

*b0* = the intercept of the line

*x1* = an independent variable

*b1* = regression coefficient

*x2* = another independent variable

*b2* = another regression coefficient

**Example**
```{r}
lm2 = lm(Weight~Length3 + Height, data = Fish)
lm2
```
Our multiple regression equation is:

*Weight = -501.1 + 25.24(Length3) + 12.14(Height)*


*****


## Measuring the Accuracy/"Fitness" of a Linear Predictive Model

There are multiple ways to measure the accuracy of a predictive model, based on the set of data you are given. The more accurate the model, the better it will do at predicting future events.

**Correlation Coefficient**
One very basic measurement of "fitness" for a model is the correlation coefficient. The correlation coefficient (r) is a measure of the strength of the linear relationship between two variables. It also gives the direction of the relationship (positive or negative).

**Coefficient of Determination**
Denoted as r^2, the coefficient of determination is interpreted as the proportion of the variance in the dependent variable (ŷ) that is predictable from the independent variable(s). That is, how much of ŷ is explained by the x('s). It is the square of the correlation coefficient.

**Residuals**
Residuals are the measure of how far away each point in a dataset is from the predicted model. The smaller the |residual| is, the closer that observation falls to the predictive model.

*res = (Y-Yest)*

*Y* = observed value

*Yest* = predicted value

```{r, eval = FALSE}
# Activity 1
data_ex = data.frame("x" = c(1, 2, 4, 6, 7), "y" = c(1, 2.5, 6, 5, 4.5))

ggplot(data = data_ex, aes(x = x, y = y))+
  geom_point(aes(color = "hotpink"), size = 3.5)+
  geom_abline(mapping = NULL, data = NULL, slope = 1, intercept = 0, aes(color = "darkgray"), size = 1.5)+
  scale_x_continuous(name = "", limits = c(0,8), breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8))+
  scale_y_continuous(name = "", limits = c(0,8), breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8))+
  theme_economist_white()+
  theme(legend.position ="none")+
  geom_text(label = "y = x", x = 4, y = 3.25, size = 7)+
  geom_text(label = "(4,6)", x = 4, y = 6.5)+
  geom_text(label = "(2,2.5)", x = 2, y = 3)+
  geom_text(label = "(1,1)", x = 1, y = 1.5)+
  geom_text(label = "(6,5)", x = 6, y = 4.5)+
  geom_text(label = "(7,5)", x = 7, y = 5)
```

**Residual Standard Error**
The residual standard error (or residual standard deviation) is the measure of the standard deviation of the residuals. The equation is:

*Sres = sqrt[sum(Y-Yest)^2/n-2]*

*Sres* --> residual standard deviation

*Y* --> observed value

*Yest* --> predicted value

*n* --> number of observations

All of these different methods can be used to compare models, and determine which model is the **best** predictive model!

### Euclidean Norm, MSE and RMSE

The equation for euclidean norm is:

For a vector: *sqrt(∑(xi)^2)*

The equation for mean square error is:

*(1/m)∑(Y-Yest)^2*

Both methods measure the accuracy of a model, particularly how far away each point is from the predicted value(s).  

RMSE, or root mean square error, is another way to measure the accuracy of a model. It is the square root of the MSE.

**Coefficient of Determination**

```{r}
# Simple linear regression (lm1)
lm1 = lm(Weight~Length3, data = Fish)
lm1
summary(lm1)$r.sq
```

```{r}
# Multiple regression (lm2)
lm2 = lm(Weight~Length3 + Height, data = Fish)
lm2
summary(lm2)$r.sq
```

```{r}
# Multiple regression (lm3)
lm3 = lm(Weight~Length3 + Height + Width, data = Fish)
lm3
summary(lm3)$r.sq
```

```{r}
# Creates new dataset with just the variables Weight, Length3, Width and Height
Fish_new = Fish %>% 
  select(Weight, Length3, Height, Width)
```

**Correlation Matrix**

```{r}
# Correlation matrix for the variables used in the 3 models
cor(Fish_new)
```


*****


## Non-Linear Predictive Models

There are many different kinds of non-linear predictive models. Logistic regression, decision trees and random forests are just a few examples. 

### Decision Trees

Decision trees are used for both numerical and categorical targets, whereas regression is used for only numerical targets. Decision trees are a type of classification model. The accuracy of categorical decision trees can be measured using a confusion matrix, which is essentially a truth table. This plays the same role as the MSE/RMSE for regression. Regression trees use RMSE or RSE for measuring accuracy.  

**Confusion Matrix**

A confusion matrix is a truth table, that counts up the number of times a model made correct predictions. After you created the model on the training set, you test the model on the testing set, and produce a confusion matrix. 

### Random Forest

Random Forests are ultimatelty entire "forests" of decision trees. Each observation in the model is evaluated by each tree, resulting in a value at the end of the tree. Then, for categorical trees, the majority rule is applied.

For regression trees, an average is taken as the prediction value for each observations

### Logistic Regression

Logistic regression is used for predicting a categorical target, where the target is binary (0,1). This in particular is called Binary Logistic Regression.

The equation for a logistic regression model is:

*p = 1/[1+e^-(b0+b1x)]*

*b0* --> the intercept

*b1* --> the slope


*****


## Sources
<https://www.investopedia.com/terms/p/predictive-modeling.asp> 

<https://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination#:~:text=The%20coefficient%20of%20determination%20(denoted,predictable%20from%20the%20independent%20variable.&text=An%20R2%20between%200,the%20dependent%20variable%20is%20predictable.>

<https://www.kaggle.com/aungpyaeap/fish-market?select=Fish.csv>

<https://www.statisticshowto.com/mean-squared-error/> 

<https://numerics.mathdotnet.com/Distance.html>  

<https://glassboxmedicine.com/2019/02/17/measuring-performance-the-confusion-matrix/> 

<https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6> 
